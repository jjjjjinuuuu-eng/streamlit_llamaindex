import streamlit as st
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Document, Settings
from llama_index.llms.openai import OpenAI
from llama_index.readers.file import PDFReader
import os

def setup_streamlit_page():
    st.set_page_config(page_title="LlamaIndex Chat", page_icon="ğŸ¦™")
    st.title("ë¬¸ì„œ ê¸°ë°˜ RAG ì±„íŒ…")

def setup_openai_api():
    openai_api_key = st.sidebar.text_input("OpenAI API Key", type="password")
    if openai_api_key:
        os.environ["OPENAI_API_KEY"] = openai_api_key
    return openai_api_key

def initialize_llm_and_settings():
    llm = OpenAI(
        temperature=0.5,
        model="gpt-4o",
        max_tokens=512,
        context_window=4096,
    )
    Settings.llm = llm
    return llm

def process_uploaded_files(uploaded_files):
    """ì—…ë¡œë“œëœ íŒŒì¼ë“¤ì„ ì²˜ë¦¬í•˜ì—¬ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ìƒì„±"""
    documents = []
    pdf_reader = PDFReader()
    
    for file in uploaded_files:
        try:
            # íŒŒì¼ ë‚´ìš©ì„ ë°”ì´íŠ¸ë¡œ ì½ê¸°
            content = file.read()
            
            # í…ìŠ¤íŠ¸ íŒŒì¼ì¸ ê²½ìš°
            if file.type == "text/plain":
                text_content = content.decode('utf-8')
                doc = Document(text=text_content, metadata={"filename": file.name})
                documents.append(doc)
            # PDF íŒŒì¼ì¸ ê²½ìš°    
            elif file.type == "application/pdf":
                # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
                with open(f"temp_{file.name}", "wb") as f:
                    f.write(content)
                # PDF íŒŒì¼ ì½ê¸°
                pdf_docs = pdf_reader.load_data(f"temp_{file.name}")
                documents.extend(pdf_docs)
                # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                os.remove(f"temp_{file.name}")
            else:
                st.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {file.name}")
                
        except Exception as e:
            st.error(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {file.name} - {str(e)}")
            
    return documents

def initialize_chat_engine(index):
    chat_engine = index.as_chat_engine(
        chat_mode="condense_plus_context",
        verbose=True,
        system_prompt="""
        ë‹¹ì‹ ì€ ì—…ë¡œë“œëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
        ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê³ ë ¤í•˜ì—¬ ë‹µë³€í•˜ë˜, í•„ìš”í•œ ê²½ìš°ì—ë§Œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”.
        ì—…ë¡œë“œëœ ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ì—†ëŠ” ë‚´ìš©ì€ ì†”ì§íˆ ëª¨ë¥¸ë‹¤ê³  ë§ì”€í•´ì£¼ì„¸ìš”.
        """
    )
    return chat_engine

def main():
    setup_streamlit_page()
    openai_api_key = setup_openai_api()

    if not openai_api_key:
        st.warning("Please enter your OpenAI API key in the sidebar.")
        return

    # íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜ 
    st.sidebar.header("ë¬¸ì„œ ì—…ë¡œë“œ")
    uploaded_files = st.sidebar.file_uploader(
        "í…ìŠ¤íŠ¸ íŒŒì¼ì´ë‚˜ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
        accept_multiple_files=True,
        type=["txt", "pdf"]
    )

    # ì—…ë¡œë“œëœ íŒŒì¼ ì²˜ë¦¬ ë° ì¸ë±ìŠ¤ ìƒì„±
    if uploaded_files:
        with st.spinner("ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            initialize_llm_and_settings()
            documents = process_uploaded_files(uploaded_files)
            
            if documents:
                # ë¬¸ì„œ ì •ë³´ í‘œì‹œ
                st.sidebar.success(f"ì²˜ë¦¬ëœ ë¬¸ì„œ ìˆ˜: {len(documents)}")
                
                # ì¸ë±ìŠ¤ ìƒì„±
                index = VectorStoreIndex.from_documents(documents)
                st.session_state.chat_engine = initialize_chat_engine(index)
                st.success("ë¬¸ì„œ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            else:
                st.error("ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return

    # ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ì±„íŒ… ê¸°ë¡ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # ì±„íŒ… ì—”ì§„ì´ ì¤€ë¹„ëœ ê²½ìš°ì—ë§Œ ì…ë ¥ í™œì„±í™”
    if "chat_engine" in st.session_state:
        # ì‚¬ìš©ì ì…ë ¥
        if prompt := st.chat_input("ë¬´ì—‡ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?"):
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)

            # AI ì‘ë‹µ ìƒì„±
            with st.chat_message("assistant"):
                with st.spinner("Thinking..."):
                    response = st.session_state.chat_engine.chat(prompt)
                    st.markdown(response.response)
                    st.session_state.messages.append(
                        {"role": "assistant", "content": response.response}
                    )

            # ë””ë²„ê·¸ ì •ë³´
            with st.expander("Debug Info"):
                st.write("Response Type:", type(response))
                st.write("Source Nodes:", response.source_nodes if hasattr(response, 'source_nodes') else None)
    else:
        st.info("ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ë©´ ì§ˆë¬¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    # ì±„íŒ… ì´ˆê¸°í™” ë²„íŠ¼
    if st.sidebar.button("ì±„íŒ… ì´ˆê¸°í™”"):
        st.session_state.messages = []
        st.rerun()

if __name__ == "__main__":
    main()